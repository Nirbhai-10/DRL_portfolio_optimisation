{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 21:09:47,691] A new study created in memory with name: no-name-c273abc6-deb8-4c0b-b307-9a3750f83dab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MVO Weights: [ 1.99919355e-22  1.16838239e-01  1.29674746e-01  2.69604640e-01\n",
      "  1.01358024e-01  1.43953446e-22  2.20026418e-23  4.09092310e-22\n",
      "  3.82524351e-01  8.73035286e-23 -1.21645007e-23  2.03881335e-22\n",
      "  2.13469639e-22  1.49301532e-22  1.44913988e-22]\n",
      "Sum of MVO Weights: 1.0\n",
      "Starting Hyperparameter Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 21:10:04,438] Trial 0 finished with value: 608.8915634011253 and parameters: {'hidden_dim': 101, 'lr': 0.0008026126483857272, 'eps_clip': 0.1952275225762871, 'gamma': 0.9915759104524109, 'entropy_weight': 0.047265766398529724, 'num_heads': 4, 'output_dim_base': 18}. Best is trial 0 with value: 608.8915634011253.\n",
      "[I 2025-03-23 21:10:22,980] Trial 1 finished with value: 571.003688211817 and parameters: {'hidden_dim': 159, 'lr': 0.00015865847923221107, 'eps_clip': 0.15045202901488505, 'gamma': 0.9731701421968795, 'entropy_weight': 0.006550534205638664, 'num_heads': 5, 'output_dim_base': 16}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:10:43,223] Trial 2 finished with value: 593.0885285262061 and parameters: {'hidden_dim': 60, 'lr': 2.1449005747090203e-05, 'eps_clip': 0.2661656760604869, 'gamma': 0.9974223791053068, 'entropy_weight': 0.0003232665877144886, 'num_heads': 8, 'output_dim_base': 23}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:10:58,623] Trial 3 finished with value: 598.2019369643189 and parameters: {'hidden_dim': 109, 'lr': 0.00014014486064734905, 'eps_clip': 0.11986971064678113, 'gamma': 0.9535050547540148, 'entropy_weight': 0.0013412305791398575, 'num_heads': 2, 'output_dim_base': 9}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:11:17,124] Trial 4 finished with value: 593.1919019687608 and parameters: {'hidden_dim': 83, 'lr': 4.0861750034977364e-05, 'eps_clip': 0.27907807831851744, 'gamma': 0.9746631355938435, 'entropy_weight': 0.04488161948523622, 'num_heads': 6, 'output_dim_base': 29}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:11:33,679] Trial 5 finished with value: 586.6531206698404 and parameters: {'hidden_dim': 76, 'lr': 0.00024279711928573624, 'eps_clip': 0.17237808902155108, 'gamma': 0.9959343768105429, 'entropy_weight': 0.005029805516088429, 'num_heads': 4, 'output_dim_base': 20}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:11:51,286] Trial 6 finished with value: 600.0610374515811 and parameters: {'hidden_dim': 39, 'lr': 0.00022658376879009277, 'eps_clip': 0.21327342464904112, 'gamma': 0.9644319480072935, 'entropy_weight': 0.04688500542209733, 'num_heads': 6, 'output_dim_base': 20}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:12:10,806] Trial 7 finished with value: 587.7336835198753 and parameters: {'hidden_dim': 204, 'lr': 2.567141210712756e-06, 'eps_clip': 0.17368976645839004, 'gamma': 0.9796921536645403, 'entropy_weight': 0.0020200609082693106, 'num_heads': 7, 'output_dim_base': 22}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:12:29,039] Trial 8 finished with value: 601.4860573707217 and parameters: {'hidden_dim': 75, 'lr': 0.00018139711231265624, 'eps_clip': 0.2870146290094406, 'gamma': 0.9698441601500194, 'entropy_weight': 0.0001150360761994068, 'num_heads': 7, 'output_dim_base': 12}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:12:48,370] Trial 9 finished with value: 606.4237427691583 and parameters: {'hidden_dim': 223, 'lr': 3.584367715702758e-05, 'eps_clip': 0.26404970654982235, 'gamma': 0.9634168121873038, 'entropy_weight': 0.050588260215494535, 'num_heads': 7, 'output_dim_base': 13}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:13:04,445] Trial 10 finished with value: 602.2931229229711 and parameters: {'hidden_dim': 166, 'lr': 4.157948298893284e-06, 'eps_clip': 0.10227988172468502, 'gamma': 0.98434409914575, 'entropy_weight': 0.008388211129128305, 'num_heads': 2, 'output_dim_base': 32}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:13:21,963] Trial 11 finished with value: 589.563958306295 and parameters: {'hidden_dim': 149, 'lr': 0.0008856456599684249, 'eps_clip': 0.14873691540577416, 'gamma': 0.9872868306509551, 'entropy_weight': 0.007295177861483719, 'num_heads': 4, 'output_dim_base': 17}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:13:39,332] Trial 12 finished with value: 596.6701204262919 and parameters: {'hidden_dim': 174, 'lr': 8.75588567526542e-05, 'eps_clip': 0.1511283593863009, 'gamma': 0.9984574795021409, 'entropy_weight': 0.0066328797625992055, 'num_heads': 4, 'output_dim_base': 26}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:13:56,823] Trial 13 finished with value: 592.1394511696159 and parameters: {'hidden_dim': 120, 'lr': 0.0003221691357373491, 'eps_clip': 0.22606022027552874, 'gamma': 0.9756026675587826, 'entropy_weight': 0.014514668701670373, 'num_heads': 5, 'output_dim_base': 15}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:14:13,745] Trial 14 finished with value: 597.709601290015 and parameters: {'hidden_dim': 249, 'lr': 9.712005215034953e-06, 'eps_clip': 0.15391414859640837, 'gamma': 0.966116734407747, 'entropy_weight': 0.0009302826652749254, 'num_heads': 3, 'output_dim_base': 8}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:14:31,704] Trial 15 finished with value: 595.8165938624202 and parameters: {'hidden_dim': 133, 'lr': 6.521560972765867e-05, 'eps_clip': 0.18747727214759102, 'gamma': 0.9556365195821782, 'entropy_weight': 0.003474722510406564, 'num_heads': 5, 'output_dim_base': 23}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:14:48,308] Trial 16 finished with value: 601.4674257187348 and parameters: {'hidden_dim': 192, 'lr': 0.00044468592417608793, 'eps_clip': 0.2307294501700774, 'gamma': 0.9788995598555671, 'entropy_weight': 0.0005502182719448281, 'num_heads': 3, 'output_dim_base': 18}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:15:05,000] Trial 17 finished with value: 595.5681608327509 and parameters: {'hidden_dim': 149, 'lr': 1.179141648096014e-05, 'eps_clip': 0.12908772980336986, 'gamma': 0.9920886594669415, 'entropy_weight': 0.021437782911355913, 'num_heads': 3, 'output_dim_base': 26}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:15:22,663] Trial 18 finished with value: 600.9936759495235 and parameters: {'hidden_dim': 45, 'lr': 9.952545501366602e-05, 'eps_clip': 0.1734554888584409, 'gamma': 0.9844082104033904, 'entropy_weight': 0.0031163724225670266, 'num_heads': 6, 'output_dim_base': 14}. Best is trial 1 with value: 571.003688211817.\n",
      "[I 2025-03-23 21:15:39,861] Trial 19 finished with value: 615.0143000778756 and parameters: {'hidden_dim': 95, 'lr': 0.00043868591322293815, 'eps_clip': 0.13393289718395476, 'gamma': 0.9582709223932627, 'entropy_weight': 0.01922271992109258, 'num_heads': 5, 'output_dim_base': 11}. Best is trial 1 with value: 571.003688211817.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 159, 'lr': 0.00015865847923221107, 'eps_clip': 0.15045202901488505, 'gamma': 0.9731701421968795, 'entropy_weight': 0.006550534205638664, 'num_heads': 5, 'output_dim_base': 16}\n",
      "Starting Final Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PPOAgent:\n\tUnexpected key(s) in state_dict: \"attention.attention_heads.5.weight\", \"attention.attention_heads.5.bias\", \"attention.attention_heads.6.weight\", \"attention.attention_heads.6.bias\", \"attention.attention_heads.7.weight\", \"attention.attention_heads.7.bias\". \n\tsize mismatch for attention.attention_heads.0.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.0.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.1.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.1.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.2.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.2.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.3.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.3.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.4.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.4.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for actor.0.weight: copying a param with shape torch.Size([67, 224]) from checkpoint, the shape in current model is torch.Size([159, 80]).\n\tsize mismatch for actor.0.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for actor.2.weight: copying a param with shape torch.Size([67, 67]) from checkpoint, the shape in current model is torch.Size([159, 159]).\n\tsize mismatch for actor.2.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for actor.4.weight: copying a param with shape torch.Size([30, 67]) from checkpoint, the shape in current model is torch.Size([30, 159]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([67, 224]) from checkpoint, the shape in current model is torch.Size([159, 80]).\n\tsize mismatch for critic.0.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for critic.2.weight: copying a param with shape torch.Size([67, 67]) from checkpoint, the shape in current model is torch.Size([159, 159]).\n\tsize mismatch for critic.2.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for critic.4.weight: copying a param with shape torch.Size([1, 67]) from checkpoint, the shape in current model is torch.Size([1, 159]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 377\u001b[0m\n\u001b[1;32m    375\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Final Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 377\u001b[0m train_ppo(env, ppo_agent, episodes\u001b[38;5;241m=\u001b[39mnum_episodes, writer\u001b[38;5;241m=\u001b[39mwriter, save_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    378\u001b[0m alpha_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m    379\u001b[0m combined_results \u001b[38;5;241m=\u001b[39m evaluate_combined_weights(env, ppo_agent, alpha_values)\n",
      "Cell \u001b[0;32mIn[1], line 241\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env, agent, episodes, writer, save_interval, checkpoint_path)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_ppo\u001b[39m(env, agent, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, save_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 241\u001b[0m     start_episode \u001b[38;5;241m=\u001b[39m load_checkpoint(agent, checkpoint_path)\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_episode, episodes):\n\u001b[1;32m    243\u001b[0m         state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(agent, filename)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[1;32m     37\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 38\u001b[0m     agent\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     39\u001b[0m     agent\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_actor_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m     agent\u001b[38;5;241m.\u001b[39moptimizer_critic\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_critic_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PPOAgent:\n\tUnexpected key(s) in state_dict: \"attention.attention_heads.5.weight\", \"attention.attention_heads.5.bias\", \"attention.attention_heads.6.weight\", \"attention.attention_heads.6.bias\", \"attention.attention_heads.7.weight\", \"attention.attention_heads.7.bias\". \n\tsize mismatch for attention.attention_heads.0.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.0.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.1.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.1.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.2.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.2.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.3.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.3.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for attention.attention_heads.4.weight: copying a param with shape torch.Size([28, 40]) from checkpoint, the shape in current model is torch.Size([16, 40]).\n\tsize mismatch for attention.attention_heads.4.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for actor.0.weight: copying a param with shape torch.Size([67, 224]) from checkpoint, the shape in current model is torch.Size([159, 80]).\n\tsize mismatch for actor.0.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for actor.2.weight: copying a param with shape torch.Size([67, 67]) from checkpoint, the shape in current model is torch.Size([159, 159]).\n\tsize mismatch for actor.2.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for actor.4.weight: copying a param with shape torch.Size([30, 67]) from checkpoint, the shape in current model is torch.Size([30, 159]).\n\tsize mismatch for critic.0.weight: copying a param with shape torch.Size([67, 224]) from checkpoint, the shape in current model is torch.Size([159, 80]).\n\tsize mismatch for critic.0.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for critic.2.weight: copying a param with shape torch.Size([67, 67]) from checkpoint, the shape in current model is torch.Size([159, 159]).\n\tsize mismatch for critic.2.bias: copying a param with shape torch.Size([67]) from checkpoint, the shape in current model is torch.Size([159]).\n\tsize mismatch for critic.4.weight: copying a param with shape torch.Size([1, 67]) from checkpoint, the shape in current model is torch.Size([1, 159])."
     ]
    }
   ],
   "source": [
    "#!pip install cvxpy optuna\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_study(study, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "def load_study(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            study = pickle.load(f)\n",
    "        return study\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(agent, episode, filename):\n",
    "    checkpoint = {'agent_state_dict': agent.state_dict(), 'optimizer_actor_state_dict': agent.optimizer_actor.state_dict(), 'optimizer_critic_state_dict': agent.optimizer_critic.state_dict(), 'episode': episode}\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(agent, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        try:\n",
    "            checkpoint = torch.load(filename, map_location=device)\n",
    "            agent.load_state_dict(checkpoint['agent_state_dict'])\n",
    "            agent.optimizer_actor.load_state_dict(checkpoint['optimizer_actor_state_dict'])\n",
    "            agent.optimizer_critic.load_state_dict(checkpoint['optimizer_critic_state_dict'])\n",
    "            episode = checkpoint['episode']\n",
    "            print(f\"Checkpoint loaded from episode {episode}\")\n",
    "            return episode\n",
    "        except Exception as e:\n",
    "            print(\"Checkpoint exists but failed to load due to error:\", e, \"Training from scratch.\")\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def save_agent(agent, filename):\n",
    "    torch.save(agent.state_dict(), filename)\n",
    "\n",
    "def load_agent(agent, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        agent.load_state_dict(checkpoint)\n",
    "\n",
    "data = pd.read_csv('input.csv')\n",
    "stock_headers = ['INFY','BSOFT','BBOX','ACCELYA','HBLPOWER','BOSCHLTD','NCC','AUROPHARMA','NATCOPHARM','SHRIRAMFIN','HINDUNILVR','SBIN','DRREDDY','BHARTIARTL','ONGC']\n",
    "bond_headers = ['IN5Y','IN10Y']\n",
    "macro_headers = ['Inflation','GDP','Unemployment','Repo Rate','Corporate Tax rate','IIP','Exchange Rate']\n",
    "tech_indicators = ['SMA_20','EMA_20','EMA_50','RSI','BB_High','BB_Low','BB_Mid','MACD','MACD_Signal','MACD_Diff','ATR','Stoch','Stoch_Signal','SMA_20_x_RSI','SMA_20_x_MACD','RSI_x_MACD']\n",
    "features = stock_headers + bond_headers + macro_headers + tech_indicators\n",
    "split_idx = int(len(data) * 0.9)\n",
    "train_data = data.iloc[:split_idx].copy()\n",
    "test_data = data.iloc[split_idx:].copy()\n",
    "train_data.loc[:, features] = train_data[features].replace([np.inf, -np.inf], np.nan)\n",
    "train_data.loc[:, features] = train_data[features].ffill().bfill()\n",
    "state_data = train_data[features].values\n",
    "price_data = train_data[stock_headers]\n",
    "daily_returns_stocks = price_data.pct_change().dropna()\n",
    "cov_matrix = daily_returns_stocks.cov().values\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, state_data, stock_indices, cov_matrix):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.state_data = state_data\n",
    "        self.stock_indices = stock_indices\n",
    "        self.n_steps, self.n_features = state_data.shape\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(len(stock_indices),), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_features,), dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.risk_free_rate = 0.03\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.cumulative_return = 1.0\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.cumulative_return = 1.0\n",
    "        return self.state_data[self.current_step]\n",
    "    def step(self, action_weights):\n",
    "        if self.current_step >= self.n_steps - 2:\n",
    "            self.done = True\n",
    "            return self.state_data[self.current_step], 0, self.done, {'cumulative_return': self.cumulative_return}\n",
    "        price_today = self.state_data[self.current_step, self.stock_indices]\n",
    "        price_next = self.state_data[self.current_step + 1, self.stock_indices]\n",
    "        returns = (price_next / price_today) - 1\n",
    "        portfolio_return = np.dot(action_weights, returns)\n",
    "        portfolio_volatility = np.sqrt(np.dot(action_weights, np.dot(self.cov_matrix, action_weights)))\n",
    "        sharpe_ratio = (portfolio_return - self.risk_free_rate) / (portfolio_volatility + 1e-10)\n",
    "        reward = sharpe_ratio - 0.01 * portfolio_volatility\n",
    "        self.cumulative_return *= (1 + portfolio_return)\n",
    "        info = {'portfolio_return': portfolio_return, 'cumulative_return': self.cumulative_return}\n",
    "        self.current_step += 1\n",
    "        return self.state_data[self.current_step], reward, self.done, info\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.ptr] = (state, action, reward, next_state, done)\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in idx])\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, output_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = output_dim\n",
    "        self.head_dim = output_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.output_dim\n",
    "        self.attention_heads = nn.ModuleList([nn.Linear(input_dim, self.head_dim) for _ in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        outputs = [head(x) for head in self.attention_heads]\n",
    "        return torch.cat(outputs, dim=-1)\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, num_heads, output_dim_base, lr, eps_clip, gamma, entropy_weight):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.attention = MultiHeadAttention(state_dim, num_heads, output_dim_base * num_heads)\n",
    "        self.actor = self.build_network(output_dim_base * num_heads, action_dim * 2, hidden_dim, final_activation=None)\n",
    "        self.critic = self.build_network(output_dim_base * num_heads, 1, hidden_dim, final_activation=None)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer_actor, step_size=100, gamma=0.9)\n",
    "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer_critic, step_size=100, gamma=0.9)\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.entropy_decay_rate = 0.995\n",
    "        self.regularization_weight = 0.001\n",
    "    def build_network(self, input_dim, output_dim, hidden_dim, final_activation=None):\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim)]\n",
    "        if final_activation is not None:\n",
    "            layers.append(final_activation)\n",
    "        return nn.Sequential(*layers)\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        attention_output = self.attention(state_tensor)\n",
    "        out = self.actor(attention_output)\n",
    "        mean, log_std = out.chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        action = normal.sample()\n",
    "        action = torch.tanh(action)\n",
    "        return action.squeeze(0).cpu().detach().numpy(), normal.log_prob(action).sum(-1), normal.entropy().sum(-1)\n",
    "    def compute_returns(self, rewards, dones):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + self.gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return torch.FloatTensor(returns)\n",
    "    def update(self, states, actions, log_probs, returns, entropies):\n",
    "        states_tensor = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions_tensor = torch.FloatTensor(np.array(actions)).to(device)\n",
    "        log_probs_tensor = torch.stack(log_probs).to(device).detach()\n",
    "        returns_tensor = torch.FloatTensor(returns).to(device)\n",
    "        entropies_tensor = torch.stack(entropies).to(device).detach()\n",
    "        attention_output = self.attention(states_tensor)\n",
    "        values = self.critic(attention_output).squeeze()\n",
    "        advantages = returns_tensor - values.detach()\n",
    "        for _ in range(10):\n",
    "            out = self.actor(attention_output)\n",
    "            mean, log_std = out.chunk(2, dim=-1)\n",
    "            std = log_std.exp()\n",
    "            normal = Normal(mean, std)\n",
    "            new_log_probs = normal.log_prob(actions_tensor).sum(-1)\n",
    "            ratio = torch.exp(new_log_probs - log_probs_tensor)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = ((advantages) ** 2).mean()\n",
    "            entropy_loss = -entropies_tensor.mean() * self.entropy_weight\n",
    "            l2_reg = sum(torch.norm(param) for param in self.actor.parameters())\n",
    "            loss = actor_loss + 0.5 * critic_loss + entropy_loss + self.regularization_weight * l2_reg\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "            self.optimizer_critic.step()\n",
    "            print(f\"Update: Actor Loss={actor_loss.item():.4f}, Critic Loss={critic_loss.item():.4f}, Entropy Loss={entropy_loss.item():.4f}, Total Loss={loss.item():.4f}\")\n",
    "        self.scheduler_actor.step()\n",
    "        self.scheduler_critic.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    def decay_entropy_weight(self):\n",
    "        self.entropy_weight *= self.entropy_decay_rate\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    eps_clip = trial.suggest_float(\"eps_clip\", 0.1, 0.3)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.95, 0.999)\n",
    "    entropy_weight = trial.suggest_float(\"entropy_weight\", 1e-4, 1e-1, log=True)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8)\n",
    "    output_dim_base = trial.suggest_int(\"output_dim_base\", 8, 32)\n",
    "    env_opt = PortfolioEnv(state_data, stock_indices=range(len(stock_headers)), cov_matrix=cov_matrix)\n",
    "    state_dim = state_data.shape[1]\n",
    "    action_dim = len(stock_headers)\n",
    "    agent = PPOAgent(state_dim, action_dim, hidden_dim, num_heads, output_dim_base, lr, eps_clip, gamma, entropy_weight).to(device)\n",
    "    episodes = 50\n",
    "    total_reward = 0.0\n",
    "    for ep in range(episodes):\n",
    "        state = env_opt.reset()\n",
    "        ep_rewards = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob, entropy = agent.select_action(state)\n",
    "            next_state, reward, done, info = env_opt.step(action)\n",
    "            ep_rewards.append(reward)\n",
    "            state = next_state\n",
    "        total_reward += np.sum(ep_rewards)\n",
    "    avg_reward = total_reward / episodes\n",
    "    return -avg_reward\n",
    "\n",
    "def run_hyperparameter_optimization(n_trials=20):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def train_ppo(env, agent, episodes=1000, writer=None, save_interval=10, checkpoint_path=\"ppo_checkpoint.pth\"):\n",
    "    start_episode = load_checkpoint(agent, checkpoint_path)\n",
    "    for ep in range(start_episode, episodes):\n",
    "        state = env.reset()\n",
    "        states, actions, log_probs, rewards, dones, entropies = [], [], [], [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob, entropy = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            entropies.append(entropy)\n",
    "            state = next_state\n",
    "        returns = agent.compute_returns(rewards, dones)\n",
    "        agent.update(states, actions, log_probs, returns, entropies)\n",
    "        episode_reward = np.sum(rewards)\n",
    "        print(f\"Episode {ep}: RL Reward Sum = {episode_reward:.4f}, Portfolio Value = {info.get('cumulative_return', np.nan):.4f}\")\n",
    "        if writer:\n",
    "            writer.add_scalar(\"RL Reward\", episode_reward, ep)\n",
    "            writer.add_scalar(\"Portfolio Value\", info.get('cumulative_return', 0), ep)\n",
    "        if ep % save_interval == 0:\n",
    "            save_checkpoint(agent, ep, checkpoint_path)\n",
    "        agent.decay_entropy_weight()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def mean_variance_optimization(expected_returns, cov_matrix, risk_aversion=1):\n",
    "    n = len(expected_returns)\n",
    "    w = cp.Variable(n)\n",
    "    objective = cp.Maximize(expected_returns.T @ w - risk_aversion * cp.quad_form(w, cov_matrix))\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    weights = w.value\n",
    "    weights /= np.sum(weights)\n",
    "    return weights\n",
    "\n",
    "mvo_file_path = 'MVO.csv'\n",
    "mvo_data = pd.read_csv(mvo_file_path)\n",
    "daily_returns_mvo = mvo_data[stock_headers].pct_change().dropna()\n",
    "expected_returns_mvo = daily_returns_mvo.mean().values\n",
    "cov_matrix_mvo = daily_returns_mvo.cov().values\n",
    "mvo_weights = mean_variance_optimization(expected_returns_mvo, cov_matrix_mvo)\n",
    "print(\"MVO Weights:\", mvo_weights)\n",
    "print(\"Sum of MVO Weights:\", np.sum(mvo_weights))\n",
    "\n",
    "def combine_weights(ppo_weights, mvo_weights, alpha):\n",
    "    return (1 - alpha) * ppo_weights + alpha * mvo_weights\n",
    "\n",
    "def evaluate_combined_weights(env, ppo_agent, alpha_values):\n",
    "    results = []\n",
    "    for alpha in alpha_values:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            ppo_weights, _, _ = ppo_agent.select_action(state)\n",
    "            ppo_stock_weights = ppo_weights[:len(stock_headers)]\n",
    "            combined = combine_weights(ppo_stock_weights, mvo_weights, alpha)\n",
    "            next_state, reward, done, info = env.step(combined)\n",
    "            state = next_state\n",
    "        final_value = info.get('cumulative_return', np.nan)\n",
    "        print(f\"Alpha {alpha:.2f}: Final Portfolio Value = {final_value:.4f}\")\n",
    "        results.append((alpha, final_value))\n",
    "    return results\n",
    "\n",
    "def calculate_performance_metrics(returns):\n",
    "    metrics = {}\n",
    "    metrics['ROI'] = np.sum(returns)\n",
    "    metrics['Sharpe Ratio'] = np.mean(returns) / (np.std(returns) + 1e-10)\n",
    "    downside_returns = returns[returns < 0]\n",
    "    metrics['Sortino Ratio'] = np.mean(returns) / (np.std(downside_returns) + 1e-10)\n",
    "    cum_returns = np.cumsum(returns)\n",
    "    drawdown = np.max(cum_returns) - cum_returns\n",
    "    metrics['Maximum Drawdown'] = np.max(drawdown)\n",
    "    metrics['Calmar Ratio'] = metrics['ROI'] / (metrics['Maximum Drawdown'] + 1e-10)\n",
    "    return metrics\n",
    "\n",
    "def backtest(env, agent, alpha_values):\n",
    "    backtest_results = {}\n",
    "    for alpha in alpha_values:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        returns_list = []\n",
    "        while not done:\n",
    "            ppo_weights, _, _ = agent.select_action(state)\n",
    "            ppo_stock_weights = ppo_weights[:len(stock_headers)]\n",
    "            combined = combine_weights(ppo_stock_weights, mvo_weights, alpha)\n",
    "            next_state, reward, done, info = env.step(combined)\n",
    "            returns_list.append(info.get('portfolio_return', 0))\n",
    "            state = next_state\n",
    "        metrics = calculate_performance_metrics(np.array(returns_list))\n",
    "        backtest_results[alpha] = metrics\n",
    "        print(f\"Alpha {alpha:.2f}: Metrics = {metrics}\")\n",
    "    return backtest_results\n",
    "\n",
    "def equal_weight_portfolio(stock_headers):\n",
    "    n = len(stock_headers)\n",
    "    weights = np.ones(n) / n\n",
    "    print(\"Equal Weight Portfolio Weights:\", weights)\n",
    "    print(\"Sum of Weights:\", np.sum(weights))\n",
    "    return weights\n",
    "\n",
    "def minimum_variance_portfolio(daily_returns):\n",
    "    cov_matrix_local = daily_returns.cov().values\n",
    "    n = len(daily_returns.columns)\n",
    "    w = cp.Variable(n)\n",
    "    objective = cp.Minimize(cp.quad_form(w, cov_matrix_local))\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    weights = w.value\n",
    "    weights /= np.sum(weights)\n",
    "    print(\"Minimum Variance Portfolio Weights:\", weights)\n",
    "    print(\"Sum of Weights:\", np.sum(weights))\n",
    "    return weights\n",
    "\n",
    "def evaluate_static_portfolio(env, weights):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    portfolio_returns = []\n",
    "    while not done:\n",
    "        state, _, done, info = env.step(weights)\n",
    "        portfolio_returns.append(info.get('portfolio_return', 0))\n",
    "    return calculate_performance_metrics(np.array(portfolio_returns))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    writer = SummaryWriter(log_dir='./tensorboard_logs')\n",
    "    env = PortfolioEnv(state_data, stock_indices=range(len(stock_headers)), cov_matrix=cov_matrix)\n",
    "    print(\"Starting Hyperparameter Optimization...\")\n",
    "    best_params = run_hyperparameter_optimization(n_trials=20)\n",
    "    print(\"Using Best Hyperparameters:\", best_params)\n",
    "    state_dim = state_data.shape[1]\n",
    "    action_dim = len(stock_headers)\n",
    "    ppo_agent = PPOAgent(state_dim, action_dim, best_params[\"hidden_dim\"], best_params[\"num_heads\"], best_params[\"output_dim_base\"], best_params[\"lr\"], best_params[\"eps_clip\"], best_params[\"gamma\"], best_params[\"entropy_weight\"]).to(device)\n",
    "    num_episodes = 1000\n",
    "    print(\"Starting Final Training...\")\n",
    "    train_ppo(env, ppo_agent, episodes=num_episodes, writer=writer, save_interval=10, checkpoint_path=\"ppo_checkpoint.pth\")\n",
    "    alpha_values = np.linspace(0, 1, 11)\n",
    "    combined_results = evaluate_combined_weights(env, ppo_agent, alpha_values)\n",
    "    results_df = pd.DataFrame(combined_results, columns=['Alpha', 'Final Portfolio Value'])\n",
    "    results_df.to_csv('combined_weights_results.csv', index=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(results_df['Alpha'], results_df['Final Portfolio Value'], marker='o')\n",
    "    plt.title('Final Portfolio Value for Different Alpha Values (PPO-MVO Combination)')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel('Final Portfolio Value')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('combined_weights_performance.png')\n",
    "    plt.show()\n",
    "    backtest_results = backtest(env, ppo_agent, alpha_values)\n",
    "    backtest_df = pd.DataFrame(backtest_results).T\n",
    "    backtest_df.to_csv('backtest_results.csv', index=False)\n",
    "    backtest_df.plot(kind='bar', figsize=(12,8))\n",
    "    plt.title('Backtest Performance Metrics for Different Alpha Values')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('backtest_performance_metrics.png')\n",
    "    plt.show()\n",
    "    equal_weights = equal_weight_portfolio(stock_headers)\n",
    "    equal_weight_metrics = evaluate_static_portfolio(env, equal_weights)\n",
    "    min_variance_weights = minimum_variance_portfolio(daily_returns_mvo)\n",
    "    min_variance_metrics = evaluate_static_portfolio(env, min_variance_weights)\n",
    "    portfolios = ['PPO-MVO','Equal Weight','Minimum Variance']\n",
    "    metrics_list = ['ROI','Sharpe Ratio','Sortino Ratio','Maximum Drawdown','Calmar Ratio']\n",
    "    ppo_mvo_metrics = backtest_results[0.1]\n",
    "    combined_metrics = {'PPO-MVO': ppo_mvo_metrics, 'Equal Weight': equal_weight_metrics, 'Minimum Variance': min_variance_metrics}\n",
    "    combined_metrics_df = pd.DataFrame(combined_metrics, index=metrics_list)\n",
    "    for metric in metrics_list:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        combined_metrics_df.loc[metric].plot(kind='bar', legend=True)\n",
    "        plt.title(f'Comparison of {metric} Across Portfolio Techniques', fontsize=14)\n",
    "        plt.xlabel('Portfolio Technique', fontsize=12)\n",
    "        plt.ylabel(metric, fontsize=12)\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
